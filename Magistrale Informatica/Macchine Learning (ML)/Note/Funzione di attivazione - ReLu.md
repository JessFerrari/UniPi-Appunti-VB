---
Course: "[[Machine Learning (ML)]]"
tags:
  - IA
  - ML
Area: "[[Reti Neurali (NN)]]"
topic: 
SubTopic:
---
# Funzione di attivazione - ReLu
---
la __[[Funzioni di attivazione | funzione di attivazione]] Retified Linear__ (__ReLu__) è definita come 
$$Relu(x) = \max(0,x)$$

![[Pasted image 20241227064955.png]]

è una  spesso usata per la Hidden unit nelle [[Reti Neurali (NN)|reti neurali]] 