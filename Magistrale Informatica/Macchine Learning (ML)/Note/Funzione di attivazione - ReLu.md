---
Course: "[[Machine Learning (ML)]]"
tags:
  - IA
  - ML
Area: "[[Reti Neurali (NN)]]"
topic: 
SubTopic:
---
# Funzione di attivazione - ReLu
---
la __[[Funzioni di attivazione | funzione di attivazione]] Retified Linear__ (__ReLu__) è definita come 
$$Relu(x) = \max(0,x)$$

![[Pasted image 20241227064955.png]]

è una  spesso nel [[Deep Learning|deep learning]] per via della sua derivata $$Relu'= \begin{cases}
0  & if  & x<0 \\
1  & if &  x>0
\end{cases}$$ 