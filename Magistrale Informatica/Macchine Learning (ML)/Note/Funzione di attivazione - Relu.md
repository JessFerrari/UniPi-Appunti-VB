---
Course: "[[Machine Learning (ML)]]"
tags:
  - IA
Area: "[[Reti Neurali (NN)]]"
topic: 
SubTopic:
---
# Funzione di attivazione - Relu
---
la [[Funzioni|funzioni]] __Retified Linear__ (ReLu) è una [[Funzioni di attivazione | funzione di attivazione]] spesso usata per la Hidden unit nelle reti neurali 

è definita come 
$$Relu(x) = 
\begin{cases}  
0 & if \ x\leq 0  \\
x & if \  x>0 &  
\end{cases}$$

Una sua approssimazione è la [[Funzione di attivazione - SoftPlus|SoftPlus]]
